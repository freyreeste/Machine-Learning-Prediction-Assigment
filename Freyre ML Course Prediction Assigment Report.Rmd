---
title: "ML Course Prediction Assigment"
author: "Esteban R. Freyre"
date: "December 21th, 2018"
output: html_document
---
###Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement.
The goal of the project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

Download Data Sets
```{r, cache=TRUE}
training_0<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
validation_0<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(training_0,destfile = "/home/sunshine/training_0.csv",method = "curl")
download.file(validation_0,destfile = "/home/sunshine/validation_0.csv",method = "curl")

dateDownloaded<-date()

training_0<-read.csv("/home/sunshine/training_0.csv",sep = ",",header = TRUE)
validation_0<-read.csv("/home/sunshine/validation_0.csv",sep = ",",header = TRUE)
```
### Data Cleaning and Exploration
```{r ,cache=TRUE, echo=FALSE}
str(training_0)
```
There is some cleaning to be done for those characters variables, check for zero variance variables and preparing data for partioning training and test.
```{r}
training_0<- training_0[, colSums(is.na(training_0)) == 0] # remove NA values
validation_0 <- validation_0[, colSums(is.na(validation_0)) == 0] # remove NA values
```  
There are character columns and data not meaningful as predictors to be removed
```{r cache=TRUE}
classe<-training_0$classe
training_del<-grepl("^X|timestamp|window", names(training_0)) #variables not meaningful as predictors
training_1<-training_0[, !training_del]
training_2<-training_1[, sapply(training_1,is.numeric)] # variables to be changed as numeric
training_2$classe<-classe

validation_del <- grepl("^X|timestamp|window|problem_id", names(validation_0)) # variables not meaningful as predictors
validation_1<-validation_0[, !validation_del]
validation_2<-validation_1[, sapply(validation_1,is.numeric)] # variables to be changed as numeric
```
```{r message=FALSE, warning=FALSE}
library(caret)
```
Verifying changes madde for near zero variance predictors
```{r cache=TRUE}
zerovar_2<-nearZeroVar(training_2,saveMetrics= TRUE, names=TRUE, allowParallel = TRUE)
summary(zerovar_2) # there are no more nzv
summary(training_2) # check changes
```
### Use of Cross validation  

After partitioning data for training and testing using 70/30 percent for training and testing respectively some models will be fitted. Eventually the validation data set is the one already downloaded with 20 observations and will be used only once to avoid overfitting. A comparison of predictions among models is found at the conclusion of the report.

For reasons of little computational power only a handful of models will be shown when originally more were tested. Too many challenges were met when knitting the report. Thus in order to meet course deadlines and considering ${Knitr}$ limitations for complex computations the original more comprehensive report was discarded. Nevertheless similar results are achieved boostraping with trees and k-Nearest Neighbors than those obtained with Random Forest and Gradient Boosting (GBM). 

```{r cache=TRUE}
set.seed(32343) 
inTrain <- createDataPartition(y=training_2$classe, p=0.70, list=F)
Train <- training_2[inTrain, ]
Test<- training_2[-inTrain, ]
```
Dimensions of original and training dataset comparison
```{r cache=TRUE, echo=FALSE}
rbind("original dataset" = dim(training_2),"training set" = dim(Train),"testing set"=dim(Test),"validation"=dim(validation_2))
```
Principal Component Analysis
```{r cache=TRUE, include=FALSE}
Train2<-prcomp(Train[,-53],tol=0.04,retx = TRUE)
```
```{r cache=TRUE,echo=FALSE}
summary(Train2)
```
### Exploratory Plotting
```{r message=FALSE, warning=FALSE}
library(GGally)
```
```{r cache=TRUE, echo=FALSE}
h<-ggpairs(Train,mapping = aes(color = classe),columns = c(1:5),
           lower = list(
             continuous = "smooth",
             combo = "facetdensity"))
print(h)
```
### Strategy for models and samples in training

The preferable method for sampling will be boostraping with correction for replacement to avoid bias. There are predictors with high variance and with negative values thus some transformations will be made in most cases YeoJohnson.

###Setting up parallelization
```{r}
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
```
### Data Modeling

We will follow the same approach than that in the book The Elements of Statistical Learning by  Trevor,Tibshirani and Friedman. The spectrum of clasifiers is set out between linear clasifiers and non linears with k-Nearest Neighbors. Linear Discriminant AnÃ¡liss (LDA) can deliver good results even when applied incorrectly.The third model will be bagging with trees.

```{r cache=TRUE}
library(MASS)
TCtrl_lda<-trainControl(method = "boot632",search="random",allowParallel = TRUE)
set.seed(32343)
model_lda<-train(classe ~ .,method="lda",data=Train,preProcess = "YeoJohnson",tuneLength = 12, trControl = TCtrl_lda)
model_lda
pre_lda<-predict.train(model_lda,newdata=Test)
errortestlda <- 1 - as.numeric(confusionMatrix(Test$classe, pre_lda)$overall[1])
```
It is found Gausian predictors do not perform good, which it makes sence since we face a multiclass classification problem and it is more suitable for non parametric boundaries classifications.

In this example a model tree have high variance due to the correlation in the predictors.
Following a Bagging model will be pursued, finding that it succeeds in reducing the variance and hence the test error of unstable procedures like trees. Therefore the improvement is made in prediction by increasing the number of boostrap samples and computing their corresponding estimator. We should use large trees for bagging, because the variance reduction due to bagging
asks for a large tree to balance the bias-variance trade-off.

```{r cache=TRUE}
ctrol_bag1 <- trainControl(method="boot632",search="random",allowParallel = TRUE)

set.seed(32343)
model_bag1<-train(x=Train[1:52],y=Train$classe, method="treebag",preProcess= "YeoJohnson",trControl=ctrol_bag1,tuneLength=15)
model_bag1

pre_bag1<-predict.train(model_bag1,newdata=Test)
errortestbag1 <- 1 - as.numeric(confusionMatrix(Test$classe, pre_bag1)$overall[1])
```
```{r cache=TRUE}
bag1Imp <- varImp(model_bag1)
plot(bag1Imp)
densityplot(model_bag1, pch = "|")
```

k-Nearest Neighbors
```{r cache=TRUE}
TCtrlknn<-trainControl(method = "boot632",allowParallel = FALSE)
set.seed(32343)
model_knn<-train(classe ~ ., data=Train,method="knn",preProcess = "YeoJohnson",trControl = TCtrlknn) # using preprocessing with YeoJohnson transformation because of the negative values some predictors have.
model_knn
pre_knn<-predict.train(model_knn,newdata=Test)
errortestknn <- 1 - as.numeric(confusionMatrix(Test$classe, pre_knn)$overall[1])
```

```{r cache=TRUE}
compare_results<-list(rbind(BAG=errortestbag1,KNN=errortestknn,LDA=errortestlda))
compare_results
```
Bagged CART provides the smaller out of sample error `r errortestbag1` considering the test sample. It is followed by k-Nearest Neighbors with `r errortestknn`. If a Random Forest model where fitted, it would provide an extension of Bagging on Classification/regression trees. Random Forest improves on bagging by reducing the correlation between the sampled trees.

### Predicting for Validation Data Set with chosen model
Algo is applied to the validation data previusly cleaned. 

```{r}
set.seed(32343)
validatingbag1<-summary(predict.train(model_bag1, newdata=validation_2))

set.seed(32343)
validatingknn<-summary(predict.train(model_knn, newdata=validation_2))

set.seed(32343)
validatinglda<-summary(predict.train(model_lda, newdata=validation_2))

compare_validations<-list(rbind(validatingbag1,validatingknn,validatinglda))
compare_validations
```  
### Conclusion
We find that the most accourate models predictions are quite the same (BAG1 and KNN), KNN being slighty different with classe B and D. LDA prediction is different in four classes in one point. However considering the out of sample error and the model low accuracy it ends up with a good performance.

## Apendix and Supporting Data

### Data exploration
```{r message=FALSE, warning=FALSE, cache=TRUE}
library(dplyr)
subsetroll<-select(Train,starts_with("roll"))
summary(subsetroll)
```
#Using the heatmap function in order to find patterns and covariances
```{r cache=TRUE, echo=FALSE}
Trainheat <- data.frame(Train) %>% data.matrix
heatmap(Trainheat)
```


